{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1viPmBgwHBqTe1xCbpNzYznezfcDARaE2",
      "authorship_tag": "ABX9TyO6aEnjxL3NPKxRwRALfm4c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakelang1348/RhymePredictorRNN/blob/main/Rhyme_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries included"
      ],
      "metadata": {
        "id": "jUPCO2i39TgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import sys\n",
        "import collections\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "dDIMg580dGVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data pre-preprocessing. Converts initial large dataset containing all english words into a dataset of only single syllable english words.\n",
        "Link to dataset I used for initial pre-processing: https://github.com/open-dict-data/ipa-dict/blob/master/data/en_US.txt.\n",
        "Creates new file to output_file_path containing only single syllable words"
      ],
      "metadata": {
        "id": "bE04d3xIyT3S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAqspms5sZlk",
        "outputId": "37b201f0-963b-4801-fe90-6571097690ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single syllable words filtered and saved to the new file.\n"
          ]
        }
      ],
      "source": [
        "def is_single_syllable(ipa):\n",
        "    vowel_pattern = r'[aeiouɑɔɛɪʌʊæɐɜɞɘɵʉɨɤəɚɝʰ]'\n",
        "    vowels = re.findall(vowel_pattern, ipa, re.IGNORECASE)\n",
        "    return len(vowels) == 1\n",
        "\n",
        "def filter_single_syllable_words(input_file_path, output_file_path):\n",
        "    with open(input_file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
        "        for line in input_file:\n",
        "            word, ipa = line.strip().split('\\t')\n",
        "            if is_single_syllable(ipa):\n",
        "                output_file.write(line)\n",
        "\n",
        "input_file_path = '/content/drive/MyDrive/ling441/en_US.txt' #path to dataset from github repo\n",
        "output_file_path = '/content/drive/MyDrive/ling441/en_US_single_syllable.txt' #path to the new file\n",
        "\n",
        "filter_single_syllable_words(input_file_path, output_file_path)\n",
        "\n",
        "print(\"Single syllable words filtered and saved to the new file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second data preprocessing step. Generates sets of endings from the last vowel phoneme in a word onward, to find groups of words that should rhyme"
      ],
      "metadata": {
        "id": "Y2_ZqsnX5F7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#splits file into groups of english words and phonetic forms\n",
        "def split_file(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "      lines = file.readlines()\n",
        "    english_word = []\n",
        "    phonetic_word = []\n",
        "    for line in lines:\n",
        "      parts = line.strip().split('\\t')\n",
        "      word, phonetic = parts\n",
        "      english_word.append(word)\n",
        "      phonetic_word.append(phonetic)\n",
        "    return english_word, phonetic_word\n",
        "\n",
        "\n",
        "#maps endings of words from the last vowel sequence onward to words which contain that vowel sequence\n",
        "def get_phoneme_endings_groups(words, ipa):\n",
        "    endings = {}\n",
        "    vowels = 'aeiouɑɔɛɪʌʊæɐɜɞɘɵʉɨɤəɚɝ'\n",
        "\n",
        "    for word, phonetic in zip(words, ipa):\n",
        "        last_vowel_index = max(i for i, char in enumerate(phonetic) if char in vowels)\n",
        "        ending = phonetic[last_vowel_index:]\n",
        "        if ending not in endings:\n",
        "            endings[ending] = []\n",
        "        endings[ending].append(word)\n",
        "    return endings\n",
        "\n",
        "filepath = '/content/drive/MyDrive/ling441/en_US_single_syllable.txt'\n",
        "\n",
        "word, ipa = split_file(filepath)\n",
        "endings = get_phoneme_endings_groups(word, ipa)"
      ],
      "metadata": {
        "id": "BME2GKz45FR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print a sample to make sure it properly groups words. Can change sample size if necessary"
      ],
      "metadata": {
        "id": "-6KjpBnrVMaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_sample_endings(phoneme_endings_groups, num_samples=20):\n",
        "    print(f\"Printing {num_samples} samples from the phoneme endings groups:\\n\")\n",
        "    for i, (ending, words) in enumerate(phoneme_endings_groups.items()):\n",
        "        if i >= num_samples:\n",
        "            break\n",
        "        print(f\"Ending '{ending}': {words}\\n\")\n",
        "\n",
        "print_sample_endings(endings, num_samples=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TO0RTORVA5Z",
        "outputId": "e93affe0-2ba4-4a2b-d2ff-d942212b4625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing 10 samples from the phoneme endings groups:\n",
            "\n",
            "Ending 'əz/': [\"'cause\", \"'twas\", 'buzz', 'fuzz', 'luz', 'twas']\n",
            "\n",
            "Ending 'ɔɹs/': [\"'course\", 'borse', 'bourse', 'coarse', 'corse', 'course', 'force', 'forse', 'hoarse', 'horse', 'morse', 'morss', 'norse', 'nourse', 'sorce', 'source', 'vorce']\n",
            "\n",
            "Ending 'uz/': [\"'cuse\", \"blue's\", 'blues', \"blues'\", 'boos', 'booz', 'booze', 'brews', 'bruise', 'bruse', 'buse', 'buus', 'chews', 'choose', 'clews', 'clues', \"crew's\", 'crewes', 'crews', 'cruise', 'cruse', 'cruz', 'cruze', 'cues', 'dews', \"do's\", 'drewes', 'drews', 'druse', 'druze', 'dues', 'ewes', 'flus', 'foos', 'fuse', 'glues', 'goos', 'groos', 'guse', 'hewes', 'hews', 'hoos', 'hues', 'huse', 'jews', \"jews'\", 'joos', 'kloos', 'koos', 'kruse', 'kuse', 'kuze', \"leu's\", 'loos', 'lose', \"lou's\", 'luiz', 'luse', 'meuse', 'mewes', 'moos', 'muise', 'muse', \"news'\", 'oohs', 'ooze', 'pews', \"pru's\", \"pugh's\", 'pughs', \"q.'s\", 'q.s', \"q's\", 'queues', 'roos', 'roose', 'rues', 'ruse', 'schmooze', 'screws', 'shmooze', \"shoe's\", 'shoes', 'skewes', 'skews', 'snooze', 'soos', 'sous', 'spews', \"stew's\", 'stews', 'sues', 'tewes', 'tews', 'theus', 'throughs', \"two's\", 'twos', \"u.'s\", 'u.s', \"u's\", 'views', \"who's\", 'whose', \"woo's\", 'woos', \"wu's\", \"yew's\", 'yoos', \"yu's\", \"zoo's\", 'zoos', \"zue's\"]\n",
            "\n",
            "Ending 'əm/': [\"'em\", \"'m\", 'bluhm', 'brum', 'brumm', 'bum', 'chum', 'clum', 'come', 'crum', 'crumb', 'crumm', 'cum', 'drum', 'drumm', 'dum', 'dumb', 'dumm', 'from', 'frum', 'glum', 'grum', 'gum', 'gumm', 'hum', 'humm', 'klumb', 'krum', 'krumm', 'krumme', 'kumm', 'lum', 'lumb', 'lumm', 'maam', 'mum', 'mumm', 'mumme', 'numb', 'pflum', 'plum', 'plumb', 'rum', 'schrum', 'schum', 'schumm', 'scum', 'shrum', 'shum', 'slum', 'some', 'strum', 'stum', 'stumm', 'sum', 'swum', 'thum', 'thumb', 'thumm', 'um', 'umm', 'yum']\n",
            "\n",
            "Ending 'ɛn/': [\"'gain\", 'behn', 'behne', 'ben', 'benn', 'benne', 'bren', 'brenn', 'chen', 'chien', 'dehn', 'dehne', 'den', 'denn', 'denne', 'en', 'fehn', 'fen', 'fenn', 'gen', 'glen', 'glenn', 'gren', 'gwen', 'hehn', 'hen', 'henn', 'jen', 'jenn', 'jenne', 'kehn', 'kehne', 'ken', 'kenn', 'kren', 'krenn', 'lehn', 'lehne', 'len', 'men', 'menn', 'menne', 'n', 'n.', 'pen', 'penh', 'penn', 'phen', 'prehn', 'prenn', 'rehn', 'ren', 'renn', 'renne', 'schwenn', 'sen', 'senn', 'senne', 'shen', 'sten', 'sven', 'ten', 'tenn', 'then', 'tien', 'venn', 'venne', 'wen', 'wren', 'wrenn', 'yen', 'zen']\n",
            "\n",
            "Ending 'ən/': [\"'n\", 'bruhn', 'brun', 'brunn', 'bun', 'bunn', 'byun', 'chun', 'chunn', 'done', 'donne', 'dun', 'dunn', 'dunne', 'fun', 'grun', 'gun', 'gunn', 'huhn', 'hun', 'hunn', 'jun', 'kuhne', 'kun', 'luhn', 'lun', 'lunn', 'mun', 'munn', 'none', 'nuhn', 'nun', 'nunn', 'one', 'pun', 'run', 'shun', 'son', 'spun', 'stun', 'sun', 'thrun', 'thun', 'ton', 'tonne', 'youn', 'yun']\n",
            "\n",
            "Ending 'ɛs/': [\"'s\", 'bess', 'besse', 'bless', 'bress', 'bresse', 'ches', 'chess', 'cress', 'dress', 'es', 'ess', 'esse', 'fess', 'gess', 'gless', 'gress', 'guess', 'hess', 'hesse', 'jess', 'kess', 'kless', 'kness', 'kress', 'kresse', 'less', 'mess', 'nes', 'ness', 'pesce', 'pless', 'press', \"press'\", 'presse', 'ress', 's', 's.', 'stress', 'tess', 'tress', 'vess', 'wes', 'wess', 'yes']\n",
            "\n",
            "Ending 'ɪɫ/': [\"'til\", 'bihl', 'bil', 'bill', 'brill', 'chill', 'crill', 'dill', 'dille', 'drill', 'fil', 'fill', 'fril', 'frill', 'gil', 'gill', 'grill', 'grille', 'guill', 'hill', 'hille', 'il', 'ill', 'jil', 'jill', 'kill', 'kille', 'knill', 'krill', 'lil', 'lill', 'mil', 'mill', 'mille', 'nil', 'nill', 'phil', 'pihl', 'pil', 'pill', 'pille', 'prill', 'quill', 'rill', 'schill', 'shill', 'shrill', 'sil', 'sill', 'skill', 'spill', 'stihl', 'stil', 'still', 'stille', 'swill', 'thill', 'thrill', 'til', 'till', 'trill', 'twill', 'ville', 'wil', 'wille', 'zil', 'zill']\n",
            "\n",
            "Ending 'ɪz/': [\"'tis\", 'biz', 'buis', 'czyz', 'fizz', 'frizz', 'griz', 'kriz', 'liz', 'ms', 'ms.', 'quiz', 'riz', 'tis']\n",
            "\n",
            "Ending 'ɑɹɡ/': ['aargh', 'argh', 'barg', 'garg', 'karg', 'kharg']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomly generates rhymes and non-rhymes. Does so by randomly picking a set, and if 'rhyme' is true, then randomly gets another word from that set. If rhyme is false, randomly chooses a word from a different set. Then shuffles and creates training and testing datasets"
      ],
      "metadata": {
        "id": "TVy6drMYYGqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_full_ipa_pairs(phoneme_endings_groups, num_pairs, rhyme=True):\n",
        "    pairs = []\n",
        "    endings = list(phoneme_endings_groups.keys())\n",
        "    word1, word2 = \"\", \"\"\n",
        "    for _ in range(num_pairs):\n",
        "        if rhyme:\n",
        "            #choose random ending group\n",
        "            ending = random.choice(endings)\n",
        "            if len(phoneme_endings_groups[ending]) > 1:\n",
        "                word1, word2 = random.sample(phoneme_endings_groups[ending], 2)\n",
        "        else:\n",
        "            #choose two different ending group\n",
        "            ending1, ending2 = random.sample(endings, 2)\n",
        "            word1 = random.choice(phoneme_endings_groups[ending1])\n",
        "            word2 = random.choice(phoneme_endings_groups[ending2])\n",
        "\n",
        "        #get the full ipa form\n",
        "        ipa1 = next((ipa for w, ipa in zip(word, ipa) if w == word1), None)\n",
        "        ipa2 = next((ipa for w, ipa in zip(word, ipa) if w == word2), None)\n",
        "\n",
        "        if ipa1 and ipa2:\n",
        "            pairs.append((word1, word2, rhyme, ipa1, ipa2))\n",
        "\n",
        "        if len(pairs) >= num_pairs:\n",
        "            break\n",
        "\n",
        "    return pairs\n",
        "\n",
        "#create num_pairs rhyming pairs and num_pairs non-rhyming pairs\n",
        "rhyming_pairs_full_ipa = generate_full_ipa_pairs(endings, 5000, rhyme=True)\n",
        "non_rhyming_pairs_full_ipa = generate_full_ipa_pairs(endings, 5000, rhyme=False)\n",
        "\n",
        "#randomly shuffle the data\n",
        "random.shuffle(rhyming_pairs_full_ipa)\n",
        "random.shuffle(non_rhyming_pairs_full_ipa)\n",
        "\n",
        "#get index for splitting\n",
        "split_index_rhyme = int(len(rhyming_pairs_full_ipa) * 0.8)\n",
        "split_index_non_rhyme = int(len(non_rhyming_pairs_full_ipa) * 0.8)\n",
        "#create rhyme sets\n",
        "train_set_rhyme = rhyming_pairs_full_ipa[:split_index_rhyme]\n",
        "test_set_rhyme = rhyming_pairs_full_ipa[split_index_rhyme:]\n",
        "#create non-rhyme sets\n",
        "train_set_non_rhyme = non_rhyming_pairs_full_ipa[:split_index_non_rhyme]\n",
        "test_set_non_rhyme = non_rhyming_pairs_full_ipa[split_index_non_rhyme:]\n",
        "#combine\n",
        "balanced_train_set = train_set_rhyme + train_set_non_rhyme\n",
        "balanced_test_set = test_set_rhyme + test_set_non_rhyme\n",
        "#shuffle again\n",
        "random.shuffle(balanced_train_set)\n",
        "random.shuffle(balanced_test_set)\n",
        "\n",
        "#for displaying sets\n",
        "num_train_rhyme, num_train_non_rhyme = len(train_set_rhyme), len(train_set_non_rhyme)\n",
        "num_test_rhyme, num_test_non_rhyme = len(test_set_rhyme), len(test_set_non_rhyme)\n",
        "\n",
        "train_sample_balanced = balanced_train_set[:5]\n",
        "test_sample_balanced = balanced_test_set[:5]\n",
        "\n",
        "(num_train_rhyme, num_train_non_rhyme), (num_test_rhyme, num_test_non_rhyme), train_sample_balanced, test_sample_balanced\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RoCsWgWYFSf",
        "outputId": "322715fb-2b77-4d17-f478-d3641bb2fc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 4000),\n",
              " (1000, 1000),\n",
              " [('helmke', 'gulped', False, '/ˈhɛɫmk/', '/ˈɡəɫpt/'),\n",
              "  ('haug', 'reust', False, '/ˈhɔɡ/', '/ˈɹust/'),\n",
              "  ('mum', 'swum', True, '/ˈməm/', '/ˈswəm/'),\n",
              "  ('minced', 'winced', True, '/ˈmɪnst/', '/ˈwɪnst/'),\n",
              "  ('thoughts', \"trust's\", False, '/ˈθɔts/', '/ˈtɹəsts/')],\n",
              " [('notched', 'buerge', False, '/ˈnɑtʃt/', '/ˈbjuɹdʒ/'),\n",
              "  ('luke', 'glueck', True, '/ˈɫuk/', '/ˈɡɫuk/'),\n",
              "  ('walck', 'paulk', True, '/ˈwɔɫk/', '/ˈpɔɫk/'),\n",
              "  ('obst', 'cusp', False, '/ˈɑbst/', '/ˈkəsp/'),\n",
              "  ('broom', 'fume', True, '/ˈbɹum/', '/ˈfjum/')])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepares data for use within RNN as torch tensors"
      ],
      "metadata": {
        "id": "jLdl9QZUfgzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_char_encoding(data):\n",
        "    chars = set(ch for pair in data for ipa in pair[3:5] for ch in ipa)\n",
        "    char_to_int = {ch: i + 1 for i, ch in enumerate(sorted(chars))}\n",
        "    return char_to_int\n",
        "\n",
        "def words_to_sequences(ipas, char_to_int, max_length):\n",
        "    sequences = []\n",
        "    for ipa in ipas:\n",
        "        encoded = [char_to_int.get(ch, 0) for ch in ipa]\n",
        "        encoded += [0] * (max_length - len(encoded))\n",
        "        sequences.append(encoded)\n",
        "    return sequences\n",
        "\n",
        "char_to_int = create_char_encoding(rhyming_pairs_full_ipa + non_rhyming_pairs_full_ipa)\n",
        "\n",
        "max_ipa_length = max(len(ipa) for pair in rhyming_pairs_full_ipa + non_rhyming_pairs_full_ipa for ipa in pair[3:5])\n",
        "\n",
        "train_sequences = [(words_to_sequences(pair[3:5], char_to_int, max_ipa_length), pair[2]) for pair in balanced_train_set]\n",
        "test_sequences = [(words_to_sequences(pair[3:5], char_to_int, max_ipa_length), pair[2]) for pair in balanced_test_set]\n",
        "\n",
        "train_data = [(torch.tensor(words, dtype=torch.long), torch.tensor(label, dtype=torch.float)) for words, label in train_sequences]\n",
        "test_data = [(torch.tensor(words, dtype=torch.long), torch.tensor(label, dtype=torch.float)) for words, label in test_sequences]\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "mM_7NBsegXnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model definition"
      ],
      "metadata": {
        "id": "0gSXseEYcS4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Rhyme_Predictor(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(Rhyme_Predictor, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1, x2 = x[:, 0, :], x[:, 1, :]\n",
        "        embedded1 = self.embedding(x1)\n",
        "        embedded2 = self.embedding(x2)\n",
        "        _, (hidden1, _) = self.rnn(embedded1)\n",
        "        _, (hidden2, _) = self.rnn(embedded2)\n",
        "        hidden = torch.cat((hidden1[-1], hidden2[-1]), dim=1)\n",
        "        out = self.fc(hidden)\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "vocab_size = len(char_to_int) + 1\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 1\n",
        "num_layers = 1\n",
        "\n",
        "model = Rhyme_Predictor(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers)\n"
      ],
      "metadata": {
        "id": "NIEDn1zBb52c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training/Testing Loop"
      ],
      "metadata": {
        "id": "KUPsAgArcJ5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = .001 #most consistent lr\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, num_epochs=10): #10 epochs default\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for words, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(words)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predicted = (outputs.squeeze() > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_accuracy = correct / total\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "word_to_ipa = {word: ipa for word, ipa in zip(word, ipa)} #for printing results\n",
        "ipa_to_word = {ipa: word for ipa, word in zip(ipa, word)} #for printing results\n",
        "\n",
        "\n",
        "def test(model, test_loader, num_examples=10):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    correct_examples = []\n",
        "    incorrect_examples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for words, labels in test_loader:\n",
        "            outputs = model(words)\n",
        "            predicted = (outputs.squeeze() > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            #get examples to print\n",
        "            for i in range(words.size(0)):\n",
        "                phonetic1 = ''.join([list(char_to_int.keys())[list(char_to_int.values()).index(x)] for x in words[i, 0] if x != 0])\n",
        "                phonetic2 = ''.join([list(char_to_int.keys())[list(char_to_int.values()).index(x)] for x in words[i, 1] if x != 0])\n",
        "                word1 = ipa_to_word.get(phonetic1, \"N/A\")\n",
        "                word2 = ipa_to_word.get(phonetic2, \"N/A\")\n",
        "                example = (phonetic1, word1, phonetic2, word2, labels[i], predicted[i])\n",
        "\n",
        "                if len(correct_examples) < num_examples and predicted[i] == labels[i]:\n",
        "                    correct_examples.append(example)\n",
        "                elif len(incorrect_examples) < num_examples and predicted[i] != labels[i]:\n",
        "                    incorrect_examples.append(example)\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    #print examples\n",
        "    print(\"\\nCorrectly Predicted Examples:\")\n",
        "    for phonetic1, word1, phonetic2, word2, label, pred in correct_examples:\n",
        "        print(f\"Words: {word1} ({phonetic1}), {word2} ({phonetic2}) - Label: {label.item()}, Prediction: {pred.item()}\")\n",
        "\n",
        "    print(\"\\nIncorrectly Predicted Examples:\")\n",
        "    for phonetic1, word1, phonetic2, word2, label, pred in incorrect_examples:\n",
        "        print(f\"Words: {word1} ({phonetic1}), {word2} ({phonetic2}) - Label: {label.item()}, Prediction: {pred.item()}\")\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "_w_tiDs0cIr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, test_loader, criterion, optimizer, num_epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf0sq945dZrr",
        "outputId": "3dad59f3-2163-4f85-bf0d-bf38cc295836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25, Loss: 0.6922, Train Accuracy: 0.5186\n",
            "Epoch 2/25, Loss: 0.6856, Train Accuracy: 0.5509\n",
            "Epoch 3/25, Loss: 0.6650, Train Accuracy: 0.6109\n",
            "Epoch 4/25, Loss: 0.6247, Train Accuracy: 0.6500\n",
            "Epoch 5/25, Loss: 0.5910, Train Accuracy: 0.6833\n",
            "Epoch 6/25, Loss: 0.5560, Train Accuracy: 0.7081\n",
            "Epoch 7/25, Loss: 0.5296, Train Accuracy: 0.7285\n",
            "Epoch 8/25, Loss: 0.5029, Train Accuracy: 0.7401\n",
            "Epoch 9/25, Loss: 0.4802, Train Accuracy: 0.7654\n",
            "Epoch 10/25, Loss: 0.4555, Train Accuracy: 0.7815\n",
            "Epoch 11/25, Loss: 0.4356, Train Accuracy: 0.7997\n",
            "Epoch 12/25, Loss: 0.4125, Train Accuracy: 0.8146\n",
            "Epoch 13/25, Loss: 0.3886, Train Accuracy: 0.8271\n",
            "Epoch 14/25, Loss: 0.3627, Train Accuracy: 0.8434\n",
            "Epoch 15/25, Loss: 0.3412, Train Accuracy: 0.8589\n",
            "Epoch 16/25, Loss: 0.3184, Train Accuracy: 0.8661\n",
            "Epoch 17/25, Loss: 0.2978, Train Accuracy: 0.8782\n",
            "Epoch 18/25, Loss: 0.2736, Train Accuracy: 0.8876\n",
            "Epoch 19/25, Loss: 0.2527, Train Accuracy: 0.9022\n",
            "Epoch 20/25, Loss: 0.2455, Train Accuracy: 0.9032\n",
            "Epoch 21/25, Loss: 0.2214, Train Accuracy: 0.9107\n",
            "Epoch 22/25, Loss: 0.2039, Train Accuracy: 0.9226\n",
            "Epoch 23/25, Loss: 0.1910, Train Accuracy: 0.9277\n",
            "Epoch 24/25, Loss: 0.1751, Train Accuracy: 0.9349\n",
            "Epoch 25/25, Loss: 0.1649, Train Accuracy: 0.9371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy = test(model, test_loader, num_examples=10)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS3eoKGVqlHn",
        "outputId": "ffadfaed-d270-414a-ad66-dfc64006adb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Correctly Predicted Examples:\n",
            "Words: notched (/ˈnɑtʃt/), buerge (/ˈbjuɹdʒ/) - Label: 0.0, Prediction: 0.0\n",
            "Words: luque (/ˈɫuk/), glueck (/ˈɡɫuk/) - Label: 1.0, Prediction: 1.0\n",
            "Words: walck (/ˈwɔɫk/), paulk (/ˈpɔɫk/) - Label: 1.0, Prediction: 1.0\n",
            "Words: obst (/ˈɑbst/), cusp (/ˈkəsp/) - Label: 0.0, Prediction: 0.0\n",
            "Words: proved (/ˈpɹuvd/), moved (/ˈmuvd/) - Label: 1.0, Prediction: 1.0\n",
            "Words: bengt (/ˈbɛŋkt/), tramps (/ˈtɹæmpz/) - Label: 0.0, Prediction: 0.0\n",
            "Words: lex (/ˈɫɛks/), utz (/ˈəts/) - Label: 0.0, Prediction: 0.0\n",
            "Words: yorks (/ˈjɔɹks/), corks (/ˈkɔɹks/) - Label: 1.0, Prediction: 1.0\n",
            "Words: swooned (/ˈswund/), pruned (/ˈpɹund/) - Label: 1.0, Prediction: 1.0\n",
            "Words: eastes (/ˈists/), priests (/ˈpɹists/) - Label: 1.0, Prediction: 1.0\n",
            "\n",
            "Incorrectly Predicted Examples:\n",
            "Words: broome (/ˈbɹum/), fume (/ˈfjum/) - Label: 1.0, Prediction: 0.0\n",
            "Words: jure (/ˈdʒʊɹ/), twas (/ˈtwəz/) - Label: 0.0, Prediction: 1.0\n",
            "Words: wreak (/ˈɹik/), rang (/ˈɹæŋ/) - Label: 0.0, Prediction: 1.0\n",
            "Words: kuhns (/ˈkunz/), loons (/ˈɫunz/) - Label: 1.0, Prediction: 0.0\n",
            "Words: rox (/ˈɹɑks/), kelps (/ˈkɛɫps/) - Label: 0.0, Prediction: 1.0\n",
            "Words: klick (/ˈkɫɪk/), darth (/ˈdɑɹθ/) - Label: 0.0, Prediction: 1.0\n",
            "Words: wilks (/ˈwɪɫks/), milks (/ˈmɪɫks/) - Label: 1.0, Prediction: 0.0\n",
            "Words: jenks (/ˈdʒɛŋks/), steph (/ˈstɛf/) - Label: 0.0, Prediction: 1.0\n",
            "Words: start (/ˈstɑɹt/), waxed (/ˈwækst/) - Label: 0.0, Prediction: 1.0\n",
            "Words: fords (/ˈfɔɹdz/), wunsch (/ˈwənʃ/) - Label: 0.0, Prediction: 1.0\n",
            "\n",
            "Test Accuracy: 0.713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description of the task:\n",
        "The task is to create a RNN which predicts whether two words rhyme or not. The dataset was from https://github.com/open-dict-data/ipa-dict/blob/master/data/en_US.txt. It is a dataset containing english words and their IPA transcriptions, along with stress signfiers. This dataset was cut down into just single-syllable words to focus my training. That dataset of single-syllable words and their phonetic transcriptions was then grouped into sets of phoneme endings, where each phoneme ending was calculated from the last vowel sequence (or in this case, simply the last vowel sound) onward to the end of the word. Each set contained every word that ended with the same vowel sequence (as every word within that set would necessarily rhyme with one another). This made the preparation of data for the model trivial; creating equal numbers of random pairs of words that rhymed and did not rhyme by simply grabbing word within the same group (rhyme) or grabbing words in different group (non-rhymes). This data was converted into tensors, which was used as the input for the model. It trained on a random subset of 80% of the data, and tested on a random subset of 20% of the data. The functions were set up to be as tunable as possible, so you could set different training sizes, different hyperparameters, etc."
      ],
      "metadata": {
        "id": "BMq7KUG91gx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion of results:\n",
        "The results I've achieved were a bit underwhelming. For such a trivial task as rhyme recognition (which is a relatively simple rule-based pattern), I found that I couldn't get the model to perform much better than 70% accuracy on testing, no matter how I tuned the hyperparameters, even with the training accuracy achieving results of around 99% at the last epochs. A learning rate of .001 appeared to give the most consistent results. Increasing the sample size did indeed increase accuracy, albeit marginally. What perplexes me the most is that sometimes the model will predict two words as being rhymes even when the two words don't share a single common phoneme. Overall, though, an average of around 70% accuracy shows that the model was able to make some connections and do better than simply random guessing.\n",
        "EDIT: I realized right around the time I was going to submit that I was inputting the english words to the model instead of the IPA words. I fixed this issue and to my surprise, the accuracy did not increase by much--I achieved a max accuracy of around 76% even using the phonetic transcriptions. So, I think it is quite interesting that the model performs almost as well using the spellings as an input compared to using the pronunciations as an input."
      ],
      "metadata": {
        "id": "aYnU4B0x3X1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Challenges Faced:\n",
        "Overall, the biggest challenge I faced with this project was creating the dataset. The preprocessing for this project was not a simple task. Not only did I need to modify the training dataset I used, but then also had to modify the new dataset I created from the first one, group the words, and then find a way to create the actual trainig/testing dataset, which was a significant time consumption. Overall, the preprocessing took around 2-3x the time that actually creating/training/testing/tuning the model took. Not only was it simply time consuming to convert the original dataset into something more appropriate for this mode, but it was also strenuous in terms of design choices and determining the best way to create rhyme and non-rhyme pairs. I struggled with trying to find the best way to ensure my training and testing data included varied phonemes and phoneme groups, so I settled on simply using randomness to create my dataset and assumed that increasing the size of the dataset would produce varied results. However, this method provides no guarantee of all phonemes showing up, nor does it guarantee that any one specific phoneme doesn't show up in larger or smaller proportions than it actual appears in the dataset."
      ],
      "metadata": {
        "id": "PmleeMQV4kC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion of Resources Used:\n",
        "The first resource used was a dataset of english words and their phonetic transcriptions, including stress marks to denote which syllable in each word was stressed: https://github.com/open-dict-data/ipa-dict/blob/master/data/en_US.txt.\n",
        "I then modified this dataset and created a new one which is only the single-syllable words from that dataset. That would be the dataset I used to collect groupings of rhymes and non-rhymes, which was the actual data converted into tensors for the RNN. I used pyTorch for the RNN as that is the main ML library I am used to at this point."
      ],
      "metadata": {
        "id": "11rrpO0J6Dfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a further step:\n",
        "I think the first next step I would take in this project is changing the way preprocessing is done. While I think collecting random groupings was the easiest method and achieved fine results, I think that using something less-than-random would overall produce better results. It's possible that none of the most common phoneme endings were included in the training dataset and it's also possible that all of the least used phoneme endings were included in the training dataset, and then the testing dataset could have been only the most common phonetic endings, and vice-versa. So, I think the best next step would be to rethink how to preprocess this data using a more mathematical/probabalistic approach. Apart from that, I would certainly love to see how well this model performs with multi-syllabic words, so that is another obvious next step that could be taken."
      ],
      "metadata": {
        "id": "EVctctdF7BWY"
      }
    }
  ]
}